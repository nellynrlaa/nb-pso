# -*- coding: utf-8 -*-
"""fix Split di awal SKRIPSI NAIVE BAYES PSO NELI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XlKTjFvdojIjOGb-NfVjqSDYg76oRSMD
"""

!pip install nltk
!pip install Sastrawi
!pip install preprocessor

import pandas as pd
import re
import string
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover, ArrayDictionary
import preprocessor as p
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from google.colab import drive

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/skripsi-nelly/Dataset/Dataset Cyberbullying Cek2.csv')
df

df_clean = df.drop_duplicates(subset=['full_text'])

# Menampilkan jumlah baris sebelum dan sesudah
print("Jumlah baris sebelum menghapus duplikat:", len(df))
print("Jumlah baris setelah menghapus duplikat :", len(df_clean))

# Jika ingin menimpa DataFrame asli dan reset index
df = df_clean.reset_index(drop=True)

df[['full_text', 'label']]

"""# **Split Data**

**60:20:20**
"""

from sklearn.model_selection import train_test_split

# Misalnya df adalah DataFrame yang sudah berisi 'full_text' dan 'label'
X = df['full_text']
y = df['label']

# Step 1: Split data 30% untuk test, 70% untuk train+val
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Step 2: Dari 70% trainval, ambil 10/70 (~14.3%) untuk validasi
val_ratio = 10 / 70  # ~0.142857
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval, test_size=val_ratio, random_state=42, stratify=y_trainval
)

# Cek jumlah data
print("Jumlah data train     :", len(X_train))
print("Jumlah data validasi  :", len(X_val))
print("Jumlah data test      :", len(X_test))

"""# **Preprocessing**

-------
"""

import re
import string

def clean_text(text):
    # Lowercase
    text = text.lower()

    # Hapus mention (@username)
    text = re.sub(r'@\w+', '', text)

    # Hapus &amp;
    text = text.replace('&amp;', '')

    # Hapus <USERNAME>
    text = text.replace('<username>', '')

    # Hapus URL
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Hapus angka
    text = re.sub(r'\d+', '', text)

    # Hapus tanda baca
    text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))

    # Hapus emoji
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emotikon wajah
                           u"\U0001F300-\U0001F5FF"  # simbol & pictograf
                           u"\U0001F680-\U0001F6FF"  # transport & simbol
                           u"\U0001F1E0-\U0001F1FF"  # bendera
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)

    # Hapus spasi berlebih
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Gabungkan teks dan label menjadi DataFrame
df_train = pd.DataFrame({'text': X_train, 'label': y_train})
df_val   = pd.DataFrame({'text': X_val, 'label': y_val})
df_test  = pd.DataFrame({'text': X_test, 'label': y_test})

df_train['clean_text'] = df_train['text'].apply(clean_text)
df_val['clean_text'] = df_val['text'].apply(clean_text)
df_test['clean_text'] = df_test['text'].apply(clean_text)

df_train[['text', 'clean_text','label']]

def case_folding(text):
    return text.lower()

df_train['casefolded'] = df_train['clean_text'].apply(case_folding)
df_val['casefolded'] = df_val['clean_text'].apply(case_folding)
df_test['casefolded'] = df_test['clean_text'].apply(case_folding)

df_train[['clean_text', 'casefolded', 'label']]

kamus = pd.read_csv('/content/drive/MyDrive/skripsi-nelly/Dataset/others/Slang - 3.csv')

slang_dict = dict(zip(kamus['slang'], kamus['formal']))

def normalize_text(text):
    words = text.split()  # pecah jadi list kata
    normalized_words = [slang_dict.get(word, word) for word in words]
    return ' '.join(normalized_words)

df_train['normalized'] = df_train['casefolded'].apply(normalize_text)
df_val['normalized'] = df_val['casefolded'].apply(normalize_text)
df_test['normalized'] = df_test['casefolded'].apply(normalize_text)

df_train[['casefolded', 'normalized', 'label']]

def tokenize(text):
    return text.split()

df_train['tokens'] = df_train['normalized'].apply(tokenize)
df_val['tokens'] = df_val['normalized'].apply(tokenize)
df_test['tokens'] = df_test['normalized'].apply(tokenize)

df_train[['normalized', 'tokens', 'label']]

path_csv = '/content/drive/MyDrive/skripsi-nelly/Dataset/others/kamus stopword 2.csv'

# Baca file stopword dari file custom
stopword_df = pd.read_csv(path_csv)
stopwords_custom = set(stopword_df.iloc[:, 0].dropna().str.lower())

# Ambil stopwords dari Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
factory = StopWordRemoverFactory()
stopwords_sastrawi = set(factory.get_stop_words())

# Tambahkan stopword manual
stopwords_manual = {'buat', 'bukan', 'dong', 'nih', 'ya', 'sih', 'deh', 'loh', 'orang','anak','cuma', 'banyak'}

# Gabungkan semua stopwords
stopwords = stopwords_sastrawi.union(stopwords_custom).union(stopwords_manual)

# Fungsi hapus stopword
def hapus_stopword(token_list):
    if isinstance(token_list, list):
        return [kata for kata in token_list if kata.lower() not in stopwords]
    return token_list


df_train['tokens_nostopword'] = df_train['tokens'].apply(hapus_stopword)
df_val['tokens_nostopword'] = df_val['tokens'].apply(hapus_stopword)
df_test['tokens_nostopword'] = df_test['tokens'].apply(hapus_stopword)

df_train[['tokens', 'tokens_nostopword', 'label']]

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Inisialisasi stemmer Sastrawi
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Daftar kata yang tidak ingin di-stem
kata_dikecualikan = {
    'menikah', 'menel', 'perasaan', 'setuju', 'berantem', 'seram', 'balain', 'belain', 'sebelah',
    'belajar', 'terkesan', 'nangis', 'boti', 'tingkah', 'bertingkah', 'sekedar', 'kegiatan', 'birahi',
    'kerasukan', 'kasihan bajingan', 'pelecehan', 'menemani', 'pengangguran', 'menyerah', 'mengerti',
    'agamis', 'didikan', 'pengalaman', 'kemudian', 'mengakunya', 'pemerintah', 'perhatikan', 'silahkan',
    'penipu', 'membela', 'pelukis', 'pengemis', 'memesan', 'pengemudi', 'gacoan', 'bekerja', 'pesan',
    'keterlaluan', 'pasangan', 'pejabat', 'nilai', 'bernilai', 'sialan', 'didik', 'setara', 'bajingan',
    'memadai', 'alasan', 'pendapat', 'pengamen', 'perokok', 'seekor', 'kesurupan', 'vetis', 'pelaku',
    'berilmu', 'ambulan', 'perpustakaan', 'beruntun', 'kesempatan', 'ketikan', 'didik', 'kekurangan',
    'teriak', 'kumel', 'keles', 'pelacur', 'idaman', 'beruntung', 'penyayang', 'ciri', 'berikan',
    'perusak', 'bukan_sekedar', 'anjuran', 'patriarki', 'belasan', 'pasangan', 'tidak_mengerti',
    'dilan', 'dimas', 'penghuni', 'memberkati', 'terkuak', 'pelakor', 'lemot', 'meluk', 'meki',
    'mengolah', 'pentungan', 'pemeran', 'perhatian', 'kekurangan', 'menari', 'telan', 'sekuter',
    'tunang',  'modelan', 'bermasalah', 'semoga', 'pemain', 'penghibur', 'kelainan', 'penggemar',
    'eksis', 'siaran', 'kelakuan', 'pelajaran', 'pembantu', 'bertingkah', 'simpanan', 'rasa', 'merasa',
    'lingkungan', 'suami', 'hubungan'
}

def stem_with_exceptions(tokens):
    hasil = []
    for token in tokens:
        if token in kata_dikecualikan:
            hasil.append(token)
        else:
            hasil.append(stemmer.stem(token))
    return hasil

df_train['tokens_stemmed'] = df_train['tokens_nostopword'].apply(stem_with_exceptions)
df_val['tokens_stemmed'] = df_val['tokens_nostopword'].apply(stem_with_exceptions)
df_test['tokens_stemmed'] = df_test['tokens_nostopword'].apply(stem_with_exceptions)

df_train[['tokens_nostopword', 'tokens_stemmed', 'label']]

!pip install wordcloud

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filter data untuk label cyberbullying dari df_train
df_cyber = df_train[df_train['label'] == 'cyberbullying']

# Gabungkan semua token menjadi satu teks panjang
all_cyber_words = df_cyber['tokens_stemmed'].apply(lambda tokens: ' '.join(tokens)).str.cat(sep=' ')

# Membuat dan menampilkan WordCloud
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    collocations=False
).generate(all_cyber_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("WordCloud Komentar Cyberbullying")
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Filter data untuk label cyberbullying dari df_train
df_cyber = df_train[df_train['label'] == 'noncyberbullying']

# Gabungkan semua token menjadi satu teks panjang
all_cyber_words = df_cyber['tokens_stemmed'].apply(lambda tokens: ' '.join(tokens)).str.cat(sep=' ')

# Membuat dan menampilkan WordCloud
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    collocations=False
).generate(all_cyber_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("WordCloud Komentar Nonyberbullying")
plt.show()

import pandas as pd
from collections import Counter

# Filter hanya data dengan label cyberbullying dari df_train
cyber_df = df_train[df_train['label'] == 'cyberbullying']

# Gabungkan semua token dari kolom tokens_stemmed
all_tokens = cyber_df['tokens_stemmed'].sum()

# Hitung frekuensi kata
token_freq = Counter(all_tokens)

# Ubah ke DataFrame
vocab_df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])

# Urutkan berdasarkan frekuensi (descending)
vocab_df = vocab_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)

# Tampilkan hasil
vocab_df.head(50)  # atau tampilkan semua dengan vocab_df

import pandas as pd
from collections import Counter

# Filter hanya data dengan label cyberbullying dari df_train
cyber_df = df_train[df_train['label'] == 'noncyberbullying']

# Gabungkan semua token dari kolom tokens_stemmed
all_tokens = cyber_df['tokens_stemmed'].sum()

# Hitung frekuensi kata
token_freq = Counter(all_tokens)

# Ubah ke DataFrame
vocab_df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])

# Urutkan berdasarkan frekuensi (descending)
vocab_df = vocab_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)

# Tampilkan hasil
vocab_df.head(50)  # atau tampilkan semua dengan vocab_df

# Gabungkan list token menjadi string per dokumen
df_train['text_final'] = df_train['tokens_stemmed'].apply(lambda tokens: ' '.join(tokens))
df_val['text_final'] = df_val['tokens_stemmed'].apply(lambda tokens: ' '.join(tokens))
df_test['text_final'] = df_test['tokens_stemmed'].apply(lambda tokens: ' '.join(tokens))

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi vectorizer (bisa disesuaikan parameter)
vectorizer = TfidfVectorizer()

# Fit ke data latih dan transform
X_train_tfidf = vectorizer.fit_transform(df_train['text_final'])

# Transform data validasi dan uji
X_val_tfidf = vectorizer.transform(df_val['text_final'])
X_test_tfidf = vectorizer.transform(df_test['text_final'])

y_train = df_train['label']
y_val = df_val['label']
y_test = df_test['label']

print("Shape TF-IDF Train :", X_train_tfidf.shape)
print("Shape TF-IDF Val   :", X_val_tfidf.shape)
print("Shape TF-IDF Test  :", X_test_tfidf.shape)

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Daftar alpha yang ingin dicoba
alphas = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]

best_alpha = None
best_acc = 0
results = []

# Loop untuk uji tiap alpha
for alpha in alphas:
    model = MultinomialNB(alpha=alpha)
    model.fit(X_train_tfidf, y_train)

    # Prediksi validasi
    y_val_pred = model.predict(X_val_tfidf)
    acc = accuracy_score(y_val, y_val_pred)
    results.append((alpha, acc))

    if acc > best_acc:
        best_acc = acc
        best_alpha = alpha

# Tampilkan hasil
print("Hasil tuning alpha:")
for alpha, acc in results:
    print(f"Alpha: {alpha}, Akurasi: {acc:.4f}")

print(f"\nAlpha terbaik: {best_alpha}, Akurasi validasi: {best_acc:.4f}")

import matplotlib.pyplot as plt

alphas_plot = [x[0] for x in results]
acc_plot = [x[1] for x in results]

plt.plot(alphas_plot, acc_plot, marker='o')
plt.xlabel('Alpha')
plt.ylabel('Akurasi Validasi')
plt.title('Tuning Alpha MultinomialNB')
plt.grid(True)
plt.show()

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

nb_model = MultinomialNB(alpha=best_alpha)
nb_model.fit(X_train_tfidf, y_train)

y_val_pred = nb_model.predict(X_val_tfidf)

print("Evaluasi di Data Validasi")
print(f"Akurasi: {accuracy_score(y_val, y_val_pred):.2f}")
print(classification_report(y_val, y_val_pred))

y_test_pred = nb_model.predict(X_test_tfidf)


print("Naive Bayes")
print(f"Akurasi: {accuracy_score(y_test, y_test_pred):.2f}")
print(classification_report(y_test, y_test_pred))

import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# ========== PARAMETER PSO ==========
n_particles = 40
max_iter = 100
w = 0.8             # inertia weight
c1 = 1.8            # cognitive component
c2 = 1.8            # social component

# Ambil jumlah fitur
n_features = X_train_tfidf.shape[1]

#np.random.seed(42)

# Posisi partikel: 1 kalau fitur dipilih, 0 kalau tidak
positions = np.random.randint(0, 2, size=(n_particles, n_features))

# Kecepatan awal partikel
velocities = np.random.rand(n_particles, n_features)

# Fitness (akurasi) awal
def evaluate_fitness(position):
    # Pilih subset fitur berdasarkan posisi
    if np.sum(position) == 0:
        return 0  # hindari partikel kosong
    selected_cols = np.where(position == 1)[0]
    X_train_sel = X_train_tfidf[:, selected_cols]
    X_val_sel = X_val_tfidf[:, selected_cols]

    model = MultinomialNB(alpha=best_alpha)
    model.fit(X_train_sel, y_train)
    y_pred = model.predict(X_val_sel)
    return accuracy_score(y_val, y_pred)

# Hitung fitness awal
fitness = np.array([evaluate_fitness(p) for p in positions])

# Personal best
pbest_positions = positions.copy()
pbest_fitness = fitness.copy()

# Global best
gbest_idx = np.argmax(fitness)
gbest_position = positions[gbest_idx].copy()
gbest_fitness = fitness[gbest_idx]

for iter in range(max_iter):
    for i in range(n_particles):
        # Update velocity
        r1 = np.random.rand(n_features)
        r2 = np.random.rand(n_features)

        velocities[i] = (
            w * velocities[i]
            + c1 * r1 * (pbest_positions[i] - positions[i])
            + c2 * r2 * (gbest_position - positions[i])
        )

        velocities[i] = np.clip(velocities[i], -10, 10)

        # Update position dengan sigmoid
        sigmoid = 1 / (1 + np.exp(-velocities[i]))
        positions[i] = np.where(sigmoid > 0.5, 1, 0)

        # Evaluasi fitness baru
        fit = evaluate_fitness(positions[i])

        # Update pbest
        if fit > pbest_fitness[i]:
            pbest_positions[i] = positions[i].copy()
            pbest_fitness[i] = fit

        # Update gbest
        if fit > gbest_fitness:
            gbest_position = positions[i].copy()
            gbest_fitness = fit

    print(f"Iterasi {iter+1}/{max_iter} - Akurasi terbaik: {gbest_fitness:.4f}")

# Fitur terbaik dari PSO:
selected_features_idx = np.where(gbest_position == 1)[0]
print("Jumlah fitur terpilih:", len(selected_features_idx))

selected_features_idx = np.where(gbest_position == 1)[0]
print("Jumlah fitur terpilih oleh PSO:", len(selected_features_idx))

# Ambil subset dari data TF-IDF
X_train_sel = X_train_tfidf[:, selected_features_idx]
X_val_sel   = X_val_tfidf[:, selected_features_idx]
X_test_sel  = X_test_tfidf[:, selected_features_idx]

model_pso = MultinomialNB(alpha=best_alpha)
model_pso.fit(X_train_sel, y_train)

from sklearn.metrics import accuracy_score, classification_report

y_test_pred_pso = model_pso.predict(X_test_sel)

print("=== Naive Bayes + PSO ===")
print(f"Akurasi : {accuracy_score(y_test, y_test_pred_pso):.2f}")
print(classification_report(y_test, y_test_pred_pso))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Hitung confusion matrix
cm = confusion_matrix(y_test, y_test_pred_pso)

# Visualisasi confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Noncyberbullying', 'Cyberbullying'],
            yticklabels=['Noncyberbullying', 'Cyberbullying'])

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Naive Bayes + PSO')
plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_val_enc = le.transform(y_val)
y_test_enc = le.transform(y_test)

model_pso = MultinomialNB(alpha=best_alpha)
model_pso.fit(X_train_sel, y_train_enc)  # gunakan label yang sudah diencode

from sklearn.metrics import roc_auc_score

# Ambil probabilitas kelas positif (label 1)
y_proba_val = model_pso.predict_proba(X_val_sel)[:, 1]

# Hitung AUC pada data validasi
auc_val = roc_auc_score(y_val_enc, y_proba_val)
print("AUC Validation:", auc_val)

# Probabilitas untuk kelas positif
y_proba_test = model_pso.predict_proba(X_test_sel)[:, 1]

# Hitung AUC pada data uji
auc_test = roc_auc_score(y_test_enc, y_proba_test)
print("AUC Test:", auc_test)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_test_enc, y_proba_test)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Test')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import accuracy_score, classification_report, roc_auc_score

# 3. Prediksi probabilitas pada data validasi atau uji
y_proba_val = model_pso.predict_proba(X_val_sel)[:, 1]  # untuk data validasi
y_proba_test = model_pso.predict_proba(X_test_sel)[:, 1]  # untuk data uji

# 4. Hitung skor AUC
auc_val = roc_auc_score(y_val, y_proba_val)
auc_test = roc_auc_score(y_test, y_proba_test)

print(f"AUC (Validation): {auc_val:.4f}")
print(f"AUC (Test): {auc_test:.4f}")

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_test_enc, y_proba_test)
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Test Data')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# ROC Curve pada data uji
fpr, tpr, _ = roc_curve(y_test, y_proba_test, pos_label='noncyberbullying')
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # garis diagonal
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Test Data')
plt.legend(loc='lower right')
plt.show()

# Contoh data baru
teks_baru = ["@tanyakanrl buset baru tau dia dah nikah"]

tfidf_baru = vectorizer.transform(teks_baru)

tfidf_baru_sel = tfidf_baru[:, selected_features_idx]

# Prediksi
prediksi = model_pso.predict(tfidf_baru_sel)
print("Hasil prediksi:", prediksi)

# Ambil semua fitur dari TF-IDF
all_features = np.array(vectorizer.get_feature_names_out())

# Ambil nama-nama fitur yang terpilih oleh PSO berdasarkan indeks
selected_feature_names = all_features[selected_features_idx]

selected_features_df = pd.DataFrame({
    'index_fitur': selected_features_idx,
    'fitur_terpilih': selected_feature_names
}).sort_values(by='index_fitur').reset_index(drop=True)

print(f"Total fitur terpilih oleh PSO: {len(selected_features_df)}")
selected_features_df

import pandas as pd
import numpy as np
from collections import Counter

def fitur_terpilih_cyberbullying_dengan_index(df, selected_features_idx, vectorizer):

    all_features = np.array(vectorizer.get_feature_names_out())

    # Ambil nama fitur dari indeks PSO
    selected_tokens = all_features[selected_features_idx]

    df_fitur = pd.DataFrame({
        'index_fitur': selected_features_idx,
        'token': selected_tokens
    })

    # Filter label cyberbullying
    df_cyber = df_train[df_train['label'] == 'cyberbullying']


    semua_token = [token for sublist in df_cyber['tokens_stemmed'] for token in sublist]

    # Hitung frekuensi
    counter = Counter([token for token in semua_token if token in selected_tokens])
    print(f"Total kata hasil filtering di komentar cyberbullying : {len(counter)}")

    df_freq = pd.DataFrame(counter.items(), columns=['token', 'frequency'])

    # Gabungkan dengan index
    df_final = pd.merge(df_fitur, df_freq, on='token', how='left')

    df_final['frequency'] = df_final['frequency'].fillna(0).astype(int)
    df_final = df_final.sort_values(by='frequency', ascending=False).reset_index(drop=True)

    return df_final

df_cyber_fitur_terpilih = fitur_terpilih_cyberbullying_dengan_index(df_train, selected_features_idx, vectorizer)
df_cyber_fitur_terpilih

import pandas as pd
import numpy as np
from collections import Counter

def fitur_terpilih_cyberbullying_dengan_index(df, selected_features_idx, vectorizer):

    all_features = np.array(vectorizer.get_feature_names_out())

    # Ambil nama fitur dari indeks PSO
    selected_tokens = all_features[selected_features_idx]

    df_fitur = pd.DataFrame({
        'index_fitur': selected_features_idx,
        'token': selected_tokens
    })

    # Filter label cyberbullying
    df_cyber = df_train[df_train['label'] == 'noncyberbullying']


    semua_token = [token for sublist in df_cyber['tokens_stemmed'] for token in sublist]

    # Hitung frekuensi
    counter = Counter([token for token in semua_token if token in selected_tokens])
    print(f"Total kata hasil filtering di komentar noncyberbullying : {len(counter)}")

    df_freq = pd.DataFrame(counter.items(), columns=['token', 'frequency'])

    # Gabungkan dengan index
    df_final = pd.merge(df_fitur, df_freq, on='token', how='left')

    df_final['frequency'] = df_final['frequency'].fillna(0).astype(int)
    df_final = df_final.sort_values(by='frequency', ascending=False).reset_index(drop=True)

    return df_final

df_cyber_fitur_terpilih = fitur_terpilih_cyberbullying_dengan_index(df_train, selected_features_idx, vectorizer)
df_cyber_fitur_terpilih

import joblib
import numpy as np

# Asumsikan kamu sudah punya:
# model_pso, vectorizer, selected_features_idx, best_alpha

# Simpan model dan komponen penting
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
joblib.dump({
    'model': model_pso,
    'selected_features_idx': selected_features_idx,
    'alpha': best_alpha
}, 'nb_pso_artifacts.pkl')